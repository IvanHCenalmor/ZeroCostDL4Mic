{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ZeroCostDL4Mic_Interactive_annotations_Cellpose.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"ImJoy Interactive ML","language":"python","name":"imjoy-interactive-ml"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8KzUtudGvysh"},"source":["# **Interactive segmentation**\n","---\n","\n","<font size = 4>**Interactive segmentation** is a segmentation tool powered by deep learning and ImJoy that can be used to segment bioimages and was first published by [Ouyang *et al.* in 2021, on F1000R](https://f1000research.com/articles/10-142?s=09#ref-15).\n","\n","<font size = 4>**The Original code** is freely available in GitHub:\n","https://github.com/imjoy-team/imjoy-interactive-segmentation\n","\n","<font size = 4>**Please also cite this original paper when using or developing this notebook.**\n","\n","<font size = 12>**!!Currently, this notebook only works with Google Chrome or Firefox!!**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yzATrlv6n14H"},"source":["# **1. Install Interactive segmentation**\n","---\n"]},{"cell_type":"code","metadata":{"id":"Xaacxk6suuDP","cellView":"form"},"source":["Notebook_version = '1.13.1'\n","Network = 'Kaibu'\n","\n","\n","from builtins import any as b_any\n","\n","def get_requirements_path():\n","    # Store requirements file in 'contents' directory\n","    current_dir = os.getcwd()\n","    dir_count = current_dir.count('/') - 1\n","    path = '../' * (dir_count) + 'requirements.txt'\n","    return path\n","\n","def filter_files(file_list, filter_list):\n","    filtered_list = []\n","    for fname in file_list:\n","        if b_any(fname.split('==')[0] in s for s in filter_list):\n","            filtered_list.append(fname)\n","    return filtered_list\n","\n","def build_requirements_file(before, after):\n","    path = get_requirements_path()\n","\n","    # Exporting requirements.txt for local run\n","    !pip freeze > $path\n","\n","    # Get minimum requirements file\n","    df = pd.read_csv(path, delimiter = \"\\n\")\n","    mod_list = [m.split('.')[0] for m in after if not m in before]\n","    req_list_temp = df.values.tolist()\n","    req_list = [x[0] for x in req_list_temp]\n","\n","    # Replace with package name and handle cases where import name is different to module name\n","    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n","    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n","    filtered_list = filter_files(req_list, mod_replace_list)\n","\n","    file=open(path,'w')\n","    for item in filtered_list:\n","        file.writelines(item + '\\n')\n","\n","    file.close()\n","\n","import sys\n","before = [str(m) for m in sys.modules]\n","\n","#@markdown ##Install Interactive segmentation\n","import time\n","\n","# Start the clock to measure how long it takes\n","start = time.time()\n","\n","# !pip install -U Werkzeug==1.0.1\n","# !pip install git+https://github.com/imjoy-team/imjoy-interactive-segmentation@support-export-geojson#egg=imjoy-interactive-segmentation\n","!pip install git+https://github.com/imjoy-team/imjoy-interactive-segmentation@master#egg=imjoy-interactive-trainer\n","!python3 -m ipykernel install --user --name imjoy-interactive-ml --display-name \"ImJoy Interactive ML\"\n","\n","from imjoy_interactive_trainer.imjoy_plugin import start_interactive_segmentation\n","from imjoy_interactive_trainer.interactive_trainer import InteractiveTrainer\n","from imjoy_interactive_trainer.data_utils import download_example_dataset, mask_to_geojson\n","from imjoy_interactive_trainer.imgseg.geojson_utils import geojson_to_masks\n","\n","import os\n","import glob\n","from shutil import copyfile, rmtree\n","from tifffile import imread, imsave\n","from ipywidgets import interact\n","import ipywidgets as widgets\n","from matplotlib import pyplot as plt\n","import cv2\n","from tqdm import tqdm\n","from skimage.util.shape import view_as_windows\n","from skimage import io\n","\n","!pip install cellpose \n","from cellpose import models\n","import numpy as np\n","\n","\n","import random\n","from zipfile import ZIP_DEFLATED\n","import csv\n","import pandas as pd\n","\n","from numba import jit\n","from scipy.optimize import linear_sum_assignment\n","from collections import namedtuple\n","\n","from tabulate import tabulate\n","from astropy.visualization import simple_norm\n","import matplotlib.pyplot as plt\n","\n","from concurrent.futures import ThreadPoolExecutor\n","\n","\n","def PrepareDataAsPatches(Training_source, patch_width, patch_height, Data_tag, Training_target = None):\n","\n","  # Here we assume that the Train and Test folders are already created\n","  patch_num = 0\n","\n","  for file in tqdm(os.listdir(Training_source)):\n","    \n","    if os.path.isfile(os.path.join(Training_source, file)):\n","      img = io.imread(os.path.join(Training_source, file))\n","      _,this_ext = os.path.splitext(file)\n","\n","      if len(img.shape) == 2:\n","        # Using view_as_windows with step size equal to the patch size to ensure there is no overlap\n","        patches_img = view_as_windows(img, (patch_width, patch_height), (patch_width, patch_height))\n","        patches_img = patches_img.reshape(patches_img.shape[0]*patches_img.shape[1], patch_width, patch_height)\n","\n","      elif len(img.shape) == 3:\n","        # Using view_as_windows with step size equal to the patch size to ensure there is no overlap\n","        patches_img = view_as_windows(img, (patch_width, patch_height, img.shape[2]), (patch_width, patch_height, img.shape[2]))\n","        patches_img = patches_img.reshape(patches_img.shape[0]*patches_img.shape[1], patch_width, patch_height, img.shape[2])\n","\n","      else:\n","        patches_img = []\n","        print('Data format currently unsupported.')\n","\n","\n","      if os.path.isfile(os.path.join(Training_target, file)):\n","        # print('Mask exists!')\n","        mask_exists = True\n","        mask = io.imread(os.path.join(Training_target, file))\n","        patches_mask = view_as_windows(mask, (patch_width, patch_height), (patch_width, patch_height))\n","        patches_mask = patches_mask.reshape(patches_mask.shape[0]*patches_mask.shape[1], patch_width, patch_height)\n","      else:\n","        mask_exists = False\n","        # print('Mask does not exist!')\n","\n","      for i in range(patches_img.shape[0]):\n","        save_path = os.path.join(os.path.splitext(Training_source)[0], 'test','Patch_'+str(patch_num)+' - ' + os.path.splitext(os.path.basename(file))[0])\n","        os.mkdir(save_path)\n","        img_save_path = os.path.join(save_path, Data_tag)\n","\n","        if (len(patches_img[i].shape) == 2):\n","          this_image = np.repeat(patches_img[i][:,:,np.newaxis], repeats=3, axis=2)  \n","        else:\n","          this_image = patches_img[i]\n","\n","        # Convert to 8-bit to save as png\n","        this_image =(this_image/this_image.max()*255).astype('uint8')\n","        io.imsave(img_save_path, this_image, check_contrast = False)\n","\n","        # Save raw images patches, preserving format and bit depth\n","        img_save_path_raw = os.path.join(save_path, 'Raw_data'+this_ext)\n","        io.imsave(img_save_path_raw, patches_img[i], check_contrast = False)\n","\n","        if mask_exists:\n","          with open(os.path.join(save_path, \"annotation.json\"), \"w\", encoding=\"utf-8\") as f:\n","            f.write(mask_to_geojson(patches_mask[i],simplify_tol=None))\n","\n","        patch_num += 1\n","\n","def get_image_list(Folder_path, extension_list = ['*.jpg', '*.tif', '*.png']):\n","  image_list = []\n","  for ext in extension_list:\n","    image_list = image_list + glob.glob(Folder_path+\"/\"+ext)\n","\n","  n_files = len(image_list)\n","  print('Number of files: '+str(n_files))\n","\n","  filenames_list = []\n","  for img_name in image_list:\n","    filenames_list.append(os.path.basename(img_name))\n","\n","  return image_list, filenames_list\n","\n","# Colors for the warning messages\n","class bcolors:\n","  WARNING = '\\033[31m'\n","  NORMAL = '\\033[0m'  # white (normal)\n","\n","# Check if this is the latest version of the notebook\n","# Latest_notebook_version = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_ZeroCostDL4Mic_Release.csv\")\n","\n","# print('Notebook version: '+Notebook_version[0])\n","\n","# strlist = Notebook_version[0].split('.')\n","# Notebook_version_main = strlist[0]+'.'+strlist[1]\n","\n","# if Notebook_version_main == Latest_notebook_version.columns:\n","#   print(\"This notebook is up-to-date.\")\n","# else:\n","#   print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\"+ bcolors.NORMAL)\n","\n","All_notebook_versions = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_Notebook_versions.csv\", dtype=str)\n","print('Notebook version: '+Notebook_version)\n","Latest_Notebook_version = All_notebook_versions[All_notebook_versions[\"Notebook\"] == Network]['Version'].iloc[0]\n","print('Latest notebook version: '+Latest_Notebook_version)\n","if Notebook_version == Latest_Notebook_version:\n","  print(\"This notebook is up-to-date.\")\n","else:\n","  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n","\n","# Build requirements file for local run\n","after = [str(m) for m in sys.modules]\n","build_requirements_file(before, after)\n","\n","# ---------------------------- Display ----------------------------\n","# Displaying the time elapsed for installation\n","dt = time.time() - start\n","minutes, seconds = divmod(dt, 60) \n","hours, minutes = divmod(minutes, 60) \n","print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds,1),\"sec(s)\")\n","\n","\n","print(\"-----------\")\n","print(\"Interactive segmentation installed.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A2TvmvUFCBJo"},"source":["# **2. Complete the Colab session**\n","---"]},{"cell_type":"markdown","metadata":{"id":"9ATvHFVpCIZ2"},"source":["\n","## **2.1. Check for GPU access**\n","---\n","\n","By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n","\n","<font size = 4>Go to **Runtime -> Change the Runtime type**\n","\n","<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n","\n","<font size = 4>**Accelerator: GPU** *(Graphics processing unit)*\n"]},{"cell_type":"code","metadata":{"id":"Bo0sy_5YCQdY","cellView":"form"},"source":["#@markdown ##Run this cell to check if you have GPU access\n","\n","import tensorflow as tf\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xh2v4Wuav4fe"},"source":["## **2.2. Mount your Google Drive**\n","---\n","<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n","\n","<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. \n","\n","<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."]},{"cell_type":"code","metadata":{"id":"zL52lhh5v7tl","cellView":"form"},"source":["#@markdown ##Play the cell to connect your Google Drive to Colab\n","\n","#@markdown * Click on the URL. \n","\n","#@markdown * Sign in your Google Account. \n","\n","#@markdown * Copy the authorization code. \n","\n","#@markdown * Enter the authorization code. \n","\n","#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n","\n","# mount user's Google Drive to Google Colab.\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7dCOBJqkCuSf"},"source":["# **3. Select your parameters and paths**\n","---"]},{"cell_type":"markdown","metadata":{"id":"PE6tkvQQC06f"},"source":["## **3.1. Set and prepare dataset**\n","---\n","\n","<font size = 4>**WARNING: Currently this notebook only builds 'Grayscale' Cellpose models. So please provide only grayscale equivalent dataset. WARNING.**\n","\n","<font size = 4>**`Data_folder:`:** This is the path to the data to use for interactive annotation. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below. \n","\n","<font size = 4>**`Mask_folder:`:** **(OPTIONAL)** This is the path to the corresponding masks in case some of the files contain pre-annotated labelled masks. The masks need to have the same name as their corresponding source files. Not all files need to have a mask associated with it. If no masks are available, leave path empty.\n","\n","<font size = 4>**`Use_patch:`:** This option splits the data available into patches of **`patch_size`** x **`patch_size`**. This allows to make all data consistent and formatted. We recommend to always use this option for stability. \n","\n","<font size = 4>**`Reset_data:`:** Resetting the data will empty the training data folder and remove all the annotations available from previous uses.\n","\n","<font size = 4>**`Use_example_data:`:** This will download and use the example data provided by [Ouyang *et al.* in 2021, on F1000R](https://f1000research.com/articles/10-142?s=09#ref-15).\n"]},{"cell_type":"code","metadata":{"id":"UgRFCIVFVa86","cellView":"form"},"source":["#@markdown ###**Prepare data**\n","\n","Data_folder = \"\" #@param {type:\"string\"}\n","Mask_folder = \"\" #@param {type:\"string\"}\n","\n","#@markdown ###Split the data in small non-verlapping patches?\n","Use_patches = True #@param {type:\"boolean\"}\n","\n","patch_size =  256#@param {type:\"integer\"}\n","\n","#@markdown ###Reset the data? (**!annotations will be lost!**)\n","Reset_data = False #@param {type:\"boolean\"}\n","#@markdown ###Otherwise, use example data\n","Use_example_dataset = False #@param {type:\"boolean\"}\n","\n","\n","if Use_example_dataset:\n","  Data_folder = \"/content/data/hpa_dataset_v2\"\n","  download_example_dataset()\n","  Data_split = [\"microtubules.png\", \"er.png\", \"nuclei.png\"]\n","  channels=[2, 3]\n","\n","\n","else:\n","  Data_tag = \"data.png\" #Kaibu works best with PNGs!\n","  Data_split = [Data_tag]\n","  channels=[0, 0] # grayscale images without nuclei channel\n","\n","  if (Reset_data) and (os.path.exists(os.path.join(Data_folder, \"train\"))):\n","    rmtree(os.path.join(Data_folder, \"train\"))\n","\n","  if (Reset_data) and (os.path.exists(os.path.join(Data_folder, \"test\"))):\n","    rmtree(os.path.join(Data_folder, \"test\"))\n","\n","  if (os.path.exists(os.path.join(Data_folder, \"train\"))) and (os.path.exists(os.path.join(Data_folder, \"test\"))):\n","    print(\"Kaibu data already exist. Starting from these annotations!\")\n","  else:\n","    print(\"Creating new folders!\")\n","\n","    os.mkdir(os.path.join(Data_folder, \"train\"))\n","    os.mkdir(os.path.join(Data_folder, \"test\"))\n","\n","    if Use_patches:\n","      PrepareDataAsPatches(Data_folder, patch_size,patch_size, Data_tag, Mask_folder)\n","    else:\n","      image_list, _ = get_image_list(Data_folder)\n","      # jpeg_image_list = glob.glob(Data_folder+\"/*.jpg\")\n","      n_files = len(image_list)\n","      print(\"Total number of files: \"+str(n_files))\n","\n","      for image in image_list:\n","        save_path = os.path.join(Data_folder, \"test\", os.path.splitext(os.path.basename(image))[0])\n","        os.mkdir(save_path)\n","        copyfile(image, save_path+\"/\"+Data_tag)\n","\n","        if os.path.isfile(os.path.join(Mask_folder, os.path.basename(image))):\n","          mask = io.imread(os.path.join(Mask_folder, os.path.basename(image)))\n","          with open(os.path.join(save_path, \"annotation.json\"), \"w\", encoding=\"utf-8\") as f:\n","            f.write(mask_to_geojson(mask,simplify_tol=None))\n","\n","\n","extension_list = ['*.jpg', '*.tif', '*.png']\n","image_list, filenames_list = get_image_list(Data_folder, extension_list)\n","\n","\n","if len(filenames_list) > 0:\n","  # ------------- For display ------------\n","  print('--------------------------------------------------------------')\n","  @interact\n","  def show_example_data(name = filenames_list):\n","\n","    plt.figure(figsize=(13,10))\n","    img = io.imread(os.path.join(Data_folder, name))\n","\n","    plt.imshow(img, cmap='gray')\n","    plt.title('Source image ('+str(img.shape[0])+'x'+str(img.shape[1])+')')\n","    plt.axis('off')\n","\n","\n","\n","\n","print(\"------\")\n","print(\"Data prepared for interactive segmentation.\")\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6krdgD5IEeMC"},"source":["## **3.2. Prepare the Cellpose model**\n","---\n","\n","<font size = 4>**`Model_folder:`:** Enter the path where your model will be saved once trained (for instance your result folder).\n","\n","<font size = 4>**`Model_name:`:** Name of the model, the notebook will create a folder with this name within which the model will be saved.\n","\n","<font size = 4>**`default_diameter:`:** Indicate the diameter of the objects (cells or Nuclei) you want to segment. If you input \"0\", this parameter will be estimated automatically for each of your images.\n","\n","<font size = 4>**`model_type:`:** This is the Cellpose model that will be loaded initially and from which it will train from. This will allow to run reasonnable predictions even with no additional training data.\n","\n"]},{"cell_type":"code","metadata":{"id":"aSlO_wVmhKwo","cellView":"form"},"source":["#@markdown ###**Prepare model**\n","\n","Model_folder = \"\" #@param {type:\"string\"}\n","Model_name = \"\" #@param {type:\"string\"}\n","\n","if (os.path.exists(os.path.join(Model_folder, Model_name))):\n","  print(bcolors.WARNING +\"Model folder already exists and will be deleted at next step.\"+bcolors.NORMAL)\n","\n","\n","default_diameter =  0 #@param {type:\"number\"}\n","model_type = \"default\" #@param [\"cyto\", \"nuclei\", \"default\", \"none\",\"Own model\"]\n","\n","#@markdown ###**If using your own model, please select the path to the model**\n","own_model_path = \"\" #@param {type:\"string\"}\n","\n","\n","\n","resume = True\n","pretrained_model = None\n","\n","\n","if (model_type == \"default\"):\n","  model_type = 'cyto'\n","\n","if (model_type == \"none\"):\n","  model_type = 'cyto'\n","  resume = False\n","\n","if (model_type == \"Own model\"):\n","  model_type = None\n","  pretrained_model = own_model_path\n","\n","\n","\n","if (default_diameter == 0):\n","  default_diameter = None\n","\n","\n","print(\"------\")\n","# print(model_type)\n","# print(pretrained_model)\n","print(\"Model prepared.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpv5orLZuuDR"},"source":["#**4. Interactive segmentation**\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"j2dOSJGFGHCP"},"source":["## **4.1. Run interactive segmentation interface**\n","---\n","\n","<font size = 4> This will start the interactive segmentation interface using ImJoy and Kaibu. \n","\n","*   Get an image\n","*   Predict on that image (using the pretrained model selected above)\n","*   Edit the segmentations using the Selection and Draw tools\n","*   You can save the annotations at any time\n","*   When you're happy with the annotations, you can send the data for training\n","*   You can then start the training\n","*   Get a new image and annotate as above, send for training and repeat\n","\n","<font size = 4> The training will be running in the background as you annotate and send more training data. This will both generate high quality training data while building an increasingly good model.\n","\n","<font size = 4> The Kaibu interface can be made fullscreen by clicking on the three dots on the top right of the cell, and select the Full screen option. Then, it needs to be minimized and maximised again.\n"]},{"cell_type":"code","metadata":{"id":"02YTQ_ErW6NY","cellView":"form"},"source":["# @markdown #Start interactive segmentation interface\n","\n","# Restart the trainer if necessary\n","instance_exist = True\n","try:\n","  trainer = InteractiveTrainer.get_instance()\n","except:\n","  instance_exist = False\n","\n","if instance_exist:\n","  print(\"Trainer already exists. Restarting trainer!\")\n","  trainer.stop()\n","\n","\n","if (os.path.exists(os.path.join(Model_folder, Model_name))):\n","  print('Deleting pre-existing model folder...')\n","  rmtree(os.path.join(Model_folder, Model_name))\n","\n","os.mkdir(os.path.join(Model_folder, Model_name))\n","\n","model_config = dict(type=\"cellpose\",\n","                     model_dir=os.path.join(Model_folder, Model_name),\n","                     use_gpu=True,\n","                     channels=[0, 0],\n","                     style_on=0,\n","                     batch_size=1,\n","                     default_diameter = default_diameter,\n","                     pretrained_model = pretrained_model,\n","                     model_type = model_type,\n","                     resume = resume)\n","\n","start_interactive_segmentation(model_config,\n","                               Data_folder,\n","                               Data_split,\n","                               object_name=\"cell\",\n","                               scale_factor=1.0,\n","                               restore_test_annotation=True)\n","\n","\n","# start_interactive_segmentation(model_config,\n","#                                \"/content/DATA TEMP\",\n","#                                [\"data_image.tif\"],\n","#                                object_name=\"cell\",\n","#                                scale_factor=1.0,\n","#                                restore_test_annotation=True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kucqOfCGOpa"},"source":["## **4.2. Create masks from annotations**\n","---\n","\n","<font size = 4> This cell will allow you to create and visualise instance segmentation masks. It will be created from the annotations made from the interface and will be saved into a folder called **`Paired training dataset`** in your data folder (**`Data_folder`**). This data can be used to train another segmentation model if necessary."]},{"cell_type":"code","metadata":{"id":"fMSEf9cQ0Nb9","cellView":"form"},"source":["#@markdown ##Create and check annotated training images\n","if (os.path.exists(os.path.join(Data_folder,'Paired training dataset'))):\n","  rmtree(os.path.join(Data_folder,'Paired training dataset'))\n","\n","os.mkdir(os.path.join(Data_folder,'Paired training dataset'))\n","os.mkdir(os.path.join(Data_folder,'Paired training dataset','Images'))\n","os.mkdir(os.path.join(Data_folder,'Paired training dataset','Masks'))\n","\n","\n","dir_list = os.listdir(os.path.join(Data_folder, \"train\"))\n","# _, ext = os.path.splitext(Data_tag)\n","\n","for dir in dir_list:\n","  annotation_file = os.path.join(Data_folder, \"train\", dir, \"annotation.json\") \n","  mask_dict = geojson_to_masks(annotation_file, mask_types=[\"labels\"]) \n","  labels = mask_dict[\"labels\"]\n","  imsave(os.path.join(Data_folder, \"train\", dir, \"label.tif\"), labels)\n","\n","  imsave(os.path.join(Data_folder,'Paired training dataset','Masks', dir+\".tif\"), labels)\n","\n","\n","  file_list = os.listdir(os.path.join(Data_folder, \"train\", dir))\n","  for file in file_list:\n","    filename, this_ext = os.path.splitext(file)\n","    if filename == 'Raw_data':\n","        copyfile(os.path.join(Data_folder, \"train\", dir, file), os.path.join(Data_folder,'Paired training dataset','Images', dir+this_ext))\n","\n","  # raw_data_tag = glob.glob(dir+\"/Raw_data.*\")\n","  # print(raw_data_tag)\n","  # copyfile(os.path.join(Data_folder, \"train\", dir, Data_tag), os.path.join(Data_folder,'Paired training dataset','Images', dir+ext))\n","\n","\n","\n","\n","# ------------- For display ------------\n","print('--------------------------------------------------------------')\n","@interact\n","def show_labels(dir=dir_list):\n","  plt.figure(figsize=(13,10))\n","\n","  imgSource = cv2.imread(os.path.join(Data_folder, \"train\", dir, Data_tag))\n","  imgLabel = imread(os.path.join(Data_folder, \"train\", dir, \"label.tif\"))\n","\n","  plt.subplot(121)\n","  plt.imshow(imgSource, cmap='gray', interpolation='nearest')\n","  plt.title('Source image')\n","  plt.axis('off')\n","  plt.subplot(122)\n","  plt.imshow(imgLabel, cmap='nipy_spectral', interpolation='nearest')\n","  plt.title('Label')\n","  plt.axis('off');\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrfFxMalGyE9"},"source":["## **4.3. Stop training**\n","---\n","\n","<font size = 4> Once training has started, the training will carry on until stopped. Here, the training can be stopped. This will automatically create the final model, which can be used for Quality Control (Section 5 below) and for predictions (Section 6 below).\n"]},{"cell_type":"code","metadata":{"id":"yC-u5eKQHUHC","cellView":"form"},"source":["#@markdown ##Stop training\n","\n","# Stop the trainer if it exists\n","instance_exist = True\n","try:\n","  trainer = InteractiveTrainer.get_instance()\n","except:\n","  instance_exist = False\n","\n","print('-------------')\n","if instance_exist:\n","  print(\"Trainer stopped.\")\n","  trainer.stop()\n","else:\n","  print(\"No trainers currently running.\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krBTcbMlI91b"},"source":["# **5. Evaluate your model**\n","---\n","\n","<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model.  \n","\n","\n","<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"RcdqcaPnc5C9","cellView":"form"},"source":["# model name and path\n","#@markdown ###Do you want to assess the model you just trained ?\n","Use_the_current_trained_model = True #@param {type:\"boolean\"}\n","\n","#@markdown ###If not, indicate which model you want to assess:\n","\n","QC_model_path = \"\" #@param {type:\"string\"}\n","\n","\n","if Use_the_current_trained_model :\n","\n","  QC_model_path = Model_folder+\"/\"+Model_name+\"/final\"\n","\n","  #model = models.CellposeModel(gpu=True, pretrained_model=QC_model_path, torch=False, diam_mean=30.0, net_avg=True, device=None, residual_on=True, style_on=True, concatenation=False)\n","  model = models.CellposeModel(gpu=True, pretrained_model=QC_model_path, torch=True, diam_mean=30.0, net_avg=True, device=None, residual_on=True, style_on=True, concatenation=False)\n","\n","  QC_model_folder = os.path.dirname(QC_model_path)\n","  QC_model_name = os.path.basename(QC_model_folder)\n","  Saving_path = QC_model_folder\n","\n","  print(\"The \"+str(QC_model_name)+\" model will be evaluated\")\n","\n","else:\n","\n","  if os.path.exists(QC_model_path):\n","    model = models.CellposeModel(gpu=True, pretrained_model=QC_model_path, torch=True, diam_mean=30.0, net_avg=True, device=None, residual_on=True, style_on=True, concatenation=False)\n","    \n","    QC_model_folder = os.path.dirname(QC_model_path)\n","    Saving_path = QC_model_folder\n","    QC_model_name = os.path.basename(QC_model_folder)\n","    print(\"The \"+str(QC_model_name)+\" model will be evaluated\")\n","    \n","  else:  \n","    print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!')\n","    print('Please make sure you provide a valid model path before proceeding further.')\n","\n","\n","\n","# Here we load the def that perform the QC, code taken from StarDist  https://github.com/mpicbg-csbd/stardist/blob/master/stardist/matching.py\n","\n","\n","\n","matching_criteria = dict()\n","\n","def label_are_sequential(y):\n","    \"\"\" returns true if y has only sequential labels from 1... \"\"\"\n","    labels = np.unique(y)\n","    return (set(labels)-{0}) == set(range(1,1+labels.max()))\n","\n","\n","def is_array_of_integers(y):\n","    return isinstance(y,np.ndarray) and np.issubdtype(y.dtype, np.integer)\n","\n","\n","def _check_label_array(y, name=None, check_sequential=False):\n","    err = ValueError(\"{label} must be an array of {integers}.\".format(\n","        label = 'labels' if name is None else name,\n","        integers = ('sequential ' if check_sequential else '') + 'non-negative integers',\n","    ))\n","    is_array_of_integers(y) or print(\"An error occured\")\n","    if check_sequential:\n","        label_are_sequential(y) or print(\"An error occured\")\n","    else:\n","        y.min() >= 0 or print(\"An error occured\")\n","    return True\n","\n","\n","def label_overlap(x, y, check=True):\n","    if check:\n","        _check_label_array(x,'x',True)\n","        _check_label_array(y,'y',True)\n","        x.shape == y.shape or _raise(ValueError(\"x and y must have the same shape\"))\n","    return _label_overlap(x, y)\n","\n","@jit(nopython=True)\n","def _label_overlap(x, y):\n","    x = x.ravel()\n","    y = y.ravel()\n","    overlap = np.zeros((1+x.max(),1+y.max()), dtype=np.uint)\n","    for i in range(len(x)):\n","        overlap[x[i],y[i]] += 1\n","    return overlap\n","\n","\n","def intersection_over_union(overlap):\n","    _check_label_array(overlap,'overlap')\n","    if np.sum(overlap) == 0:\n","        return overlap\n","    n_pixels_pred = np.sum(overlap, axis=0, keepdims=True)\n","    n_pixels_true = np.sum(overlap, axis=1, keepdims=True)\n","    return overlap / (n_pixels_pred + n_pixels_true - overlap)\n","\n","matching_criteria['iou'] = intersection_over_union\n","\n","\n","def intersection_over_true(overlap):\n","    _check_label_array(overlap,'overlap')\n","    if np.sum(overlap) == 0:\n","        return overlap\n","    n_pixels_true = np.sum(overlap, axis=1, keepdims=True)\n","    return overlap / n_pixels_true\n","\n","matching_criteria['iot'] = intersection_over_true\n","\n","\n","def intersection_over_pred(overlap):\n","    _check_label_array(overlap,'overlap')\n","    if np.sum(overlap) == 0:\n","        return overlap\n","    n_pixels_pred = np.sum(overlap, axis=0, keepdims=True)\n","    return overlap / n_pixels_pred\n","\n","matching_criteria['iop'] = intersection_over_pred\n","\n","\n","def precision(tp,fp,fn):\n","    return tp/(tp+fp) if tp > 0 else 0\n","def recall(tp,fp,fn):\n","    return tp/(tp+fn) if tp > 0 else 0\n","def accuracy(tp,fp,fn):\n","    # also known as \"average precision\" (?)\n","    # -> https://www.kaggle.com/c/data-science-bowl-2018#evaluation\n","    return tp/(tp+fp+fn) if tp > 0 else 0\n","def f1(tp,fp,fn):\n","    # also known as \"dice coefficient\"\n","    return (2*tp)/(2*tp+fp+fn) if tp > 0 else 0\n","\n","\n","def _safe_divide(x,y):\n","    return x/y if y>0 else 0.0\n","\n","def matching(y_true, y_pred, thresh=0.5, criterion='iou', report_matches=False):\n","    \"\"\"Calculate detection/instance segmentation metrics between ground truth and predicted label images.\n","    Currently, the following metrics are implemented:\n","    'fp', 'tp', 'fn', 'precision', 'recall', 'accuracy', 'f1', 'criterion', 'thresh', 'n_true', 'n_pred', 'mean_true_score', 'mean_matched_score', 'panoptic_quality'\n","    Corresponding objects of y_true and y_pred are counted as true positives (tp), false positives (fp), and false negatives (fn)\n","    whether their intersection over union (IoU) >= thresh (for criterion='iou', which can be changed)\n","    * mean_matched_score is the mean IoUs of matched true positives\n","    * mean_true_score is the mean IoUs of matched true positives but normalized by the total number of GT objects\n","    * panoptic_quality defined as in Eq. 1 of Kirillov et al. \"Panoptic Segmentation\", CVPR 2019\n","    Parameters\n","    ----------\n","    y_true: ndarray\n","        ground truth label image (integer valued)\n","        predicted label image (integer valued)\n","    thresh: float\n","        threshold for matching criterion (default 0.5)\n","    criterion: string\n","        matching criterion (default IoU)\n","    report_matches: bool\n","        if True, additionally calculate matched_pairs and matched_scores (note, that this returns even gt-pred pairs whose scores are below  'thresh')\n","    Returns\n","    -------\n","    Matching object with different metrics as attributes\n","    Examples\n","    --------\n","    >>> y_true = np.zeros((100,100), np.uint16)\n","    >>> y_true[10:20,10:20] = 1\n","    >>> y_pred = np.roll(y_true,5,axis = 0)\n","    >>> stats = matching(y_true, y_pred)\n","    >>> print(stats)\n","    Matching(criterion='iou', thresh=0.5, fp=1, tp=0, fn=1, precision=0, recall=0, accuracy=0, f1=0, n_true=1, n_pred=1, mean_true_score=0.0, mean_matched_score=0.0, panoptic_quality=0.0)\n","    \"\"\"\n","    _check_label_array(y_true,'y_true')\n","    _check_label_array(y_pred,'y_pred')\n","    y_true.shape == y_pred.shape or _raise(ValueError(\"y_true ({y_true.shape}) and y_pred ({y_pred.shape}) have different shapes\".format(y_true=y_true, y_pred=y_pred)))\n","    criterion in matching_criteria or _raise(ValueError(\"Matching criterion '%s' not supported.\" % criterion))\n","    if thresh is None: thresh = 0\n","    thresh = float(thresh) if np.isscalar(thresh) else map(float,thresh)\n","\n","    y_true, _, map_rev_true = relabel_sequential(y_true)\n","    y_pred, _, map_rev_pred = relabel_sequential(y_pred)\n","\n","    overlap = label_overlap(y_true, y_pred, check=False)\n","    scores = matching_criteria[criterion](overlap)\n","    assert 0 <= np.min(scores) <= np.max(scores) <= 1\n","\n","    # ignoring background\n","    scores = scores[1:,1:]\n","    n_true, n_pred = scores.shape\n","    n_matched = min(n_true, n_pred)\n","\n","    def _single(thr):\n","        not_trivial = n_matched > 0 and np.any(scores >= thr)\n","        if not_trivial:\n","            # compute optimal matching with scores as tie-breaker\n","            costs = -(scores >= thr).astype(float) - scores / (2*n_matched)\n","            true_ind, pred_ind = linear_sum_assignment(costs)\n","            assert n_matched == len(true_ind) == len(pred_ind)\n","            match_ok = scores[true_ind,pred_ind] >= thr\n","            tp = np.count_nonzero(match_ok)\n","        else:\n","            tp = 0\n","        fp = n_pred - tp\n","        fn = n_true - tp\n","        # assert tp+fp == n_pred\n","        # assert tp+fn == n_true\n","\n","        # the score sum over all matched objects (tp)\n","        sum_matched_score = np.sum(scores[true_ind,pred_ind][match_ok]) if not_trivial else 0.0\n","\n","        # the score average over all matched objects (tp)\n","        mean_matched_score = _safe_divide(sum_matched_score, tp)\n","        # the score average over all gt/true objects\n","        mean_true_score    = _safe_divide(sum_matched_score, n_true)\n","        panoptic_quality   = _safe_divide(sum_matched_score, tp+fp/2+fn/2)\n","\n","        stats_dict = dict (\n","            criterion          = criterion,\n","            thresh             = thr,\n","            fp                 = fp,\n","            tp                 = tp,\n","            fn                 = fn,\n","            precision          = precision(tp,fp,fn),\n","            recall             = recall(tp,fp,fn),\n","            accuracy           = accuracy(tp,fp,fn),\n","            f1                 = f1(tp,fp,fn),\n","            n_true             = n_true,\n","            n_pred             = n_pred,\n","            mean_true_score    = mean_true_score,\n","            mean_matched_score = mean_matched_score,\n","            panoptic_quality   = panoptic_quality,\n","        )\n","        if bool(report_matches):\n","            if not_trivial:\n","                stats_dict.update (\n","                    # int() to be json serializable\n","                    matched_pairs  = tuple((int(map_rev_true[i]),int(map_rev_pred[j])) for i,j in zip(1+true_ind,1+pred_ind)),\n","                    matched_scores = tuple(scores[true_ind,pred_ind]),\n","                    matched_tps    = tuple(map(int,np.flatnonzero(match_ok))),\n","                )\n","            else:\n","                stats_dict.update (\n","                    matched_pairs  = (),\n","                    matched_scores = (),\n","                    matched_tps    = (),\n","                )\n","        return namedtuple('Matching',stats_dict.keys())(*stats_dict.values())\n","\n","    return _single(thresh) if np.isscalar(thresh) else tuple(map(_single,thresh))\n","\n","\n","\n","def matching_dataset(y_true, y_pred, thresh=0.5, criterion='iou', by_image=False, show_progress=True, parallel=False):\n","    \"\"\"matching metrics for list of images, see `stardist.matching.matching`\n","    \"\"\"\n","    len(y_true) == len(y_pred) or _raise(ValueError(\"y_true and y_pred must have the same length.\"))\n","    return matching_dataset_lazy (\n","        tuple(zip(y_true,y_pred)), thresh=thresh, criterion=criterion, by_image=by_image, show_progress=show_progress, parallel=parallel,\n","    )\n","\n","\n","\n","def matching_dataset_lazy(y_gen, thresh=0.5, criterion='iou', by_image=False, show_progress=True, parallel=False):\n","\n","    expected_keys = set(('fp', 'tp', 'fn', 'precision', 'recall', 'accuracy', 'f1', 'criterion', 'thresh', 'n_true', 'n_pred', 'mean_true_score', 'mean_matched_score', 'panoptic_quality'))\n","\n","    single_thresh = False\n","    if np.isscalar(thresh):\n","        single_thresh = True\n","        thresh = (thresh,)\n","\n","    tqdm_kwargs = {}\n","    tqdm_kwargs['disable'] = not bool(show_progress)\n","    if int(show_progress) > 1:\n","        tqdm_kwargs['total'] = int(show_progress)\n","\n","    # compute matching stats for every pair of label images\n","    if parallel:\n","        fn = lambda pair: matching(*pair, thresh=thresh, criterion=criterion, report_matches=False)\n","        with ThreadPoolExecutor() as pool:\n","            stats_all = tuple(pool.map(fn, tqdm(y_gen,**tqdm_kwargs)))\n","    else:\n","        stats_all = tuple (\n","            matching(y_t, y_p, thresh=thresh, criterion=criterion, report_matches=False)\n","            for y_t,y_p in tqdm(y_gen,**tqdm_kwargs)\n","        )\n","\n","    # accumulate results over all images for each threshold separately\n","    n_images, n_threshs = len(stats_all), len(thresh)\n","    accumulate = [{} for _ in range(n_threshs)]\n","    for stats in stats_all:\n","        for i,s in enumerate(stats):\n","            acc = accumulate[i]\n","            for k,v in s._asdict().items():\n","                if k == 'mean_true_score' and not bool(by_image):\n","                    # convert mean_true_score to \"sum_matched_score\"\n","                    acc[k] = acc.setdefault(k,0) + v * s.n_true\n","                else:\n","                    try:\n","                        acc[k] = acc.setdefault(k,0) + v\n","                    except TypeError:\n","                        pass\n","\n","    # normalize/compute 'precision', 'recall', 'accuracy', 'f1'\n","    for thr,acc in zip(thresh,accumulate):\n","        set(acc.keys()) == expected_keys or _raise(ValueError(\"unexpected keys\"))\n","        acc['criterion'] = criterion\n","        acc['thresh'] = thr\n","        acc['by_image'] = bool(by_image)\n","        if bool(by_image):\n","            for k in ('precision', 'recall', 'accuracy', 'f1', 'mean_true_score', 'mean_matched_score', 'panoptic_quality'):\n","                acc[k] /= n_images\n","        else:\n","            tp, fp, fn, n_true = acc['tp'], acc['fp'], acc['fn'], acc['n_true']\n","            sum_matched_score = acc['mean_true_score']\n","\n","            mean_matched_score = _safe_divide(sum_matched_score, tp)\n","            mean_true_score    = _safe_divide(sum_matched_score, n_true)\n","            panoptic_quality   = _safe_divide(sum_matched_score, tp+fp/2+fn/2)\n","\n","            acc.update(\n","                precision          = precision(tp,fp,fn),\n","                recall             = recall(tp,fp,fn),\n","                accuracy           = accuracy(tp,fp,fn),\n","                f1                 = f1(tp,fp,fn),\n","                mean_true_score    = mean_true_score,\n","                mean_matched_score = mean_matched_score,\n","                panoptic_quality   = panoptic_quality,\n","            )\n","\n","    accumulate = tuple(namedtuple('DatasetMatching',acc.keys())(*acc.values()) for acc in accumulate)\n","    return accumulate[0] if single_thresh else accumulate\n","\n","\n","\n","# copied from scikit-image master for now (remove when part of a release)\n","def relabel_sequential(label_field, offset=1):\n","    \"\"\"Relabel arbitrary labels to {`offset`, ... `offset` + number_of_labels}.\n","    This function also returns the forward map (mapping the original labels to\n","    the reduced labels) and the inverse map (mapping the reduced labels back\n","    to the original ones).\n","    Parameters\n","    ----------\n","    label_field : numpy array of int, arbitrary shape\n","        An array of labels, which must be non-negative integers.\n","    offset : int, optional\n","        The return labels will start at `offset`, which should be\n","        strictly positive.\n","    Returns\n","    -------\n","    relabeled : numpy array of int, same shape as `label_field`\n","        The input label field with labels mapped to\n","        {offset, ..., number_of_labels + offset - 1}.\n","        The data type will be the same as `label_field`, except when\n","        offset + number_of_labels causes overflow of the current data type.\n","    forward_map : numpy array of int, shape ``(label_field.max() + 1,)``\n","        The map from the original label space to the returned label\n","        space. Can be used to re-apply the same mapping. See examples\n","        for usage. The data type will be the same as `relabeled`.\n","    inverse_map : 1D numpy array of int, of length offset + number of labels\n","        The map from the new label space to the original space. This\n","        can be used to reconstruct the original label field from the\n","        relabeled one. The data type will be the same as `relabeled`.\n","    Notes\n","    -----\n","    The label 0 is assumed to denote the background and is never remapped.\n","    The forward map can be extremely big for some inputs, since its\n","    length is given by the maximum of the label field. However, in most\n","    situations, ``label_field.max()`` is much smaller than\n","    ``label_field.size``, and in these cases the forward map is\n","    guaranteed to be smaller than either the input or output images.\n","    Examples\n","    --------\n","    >>> from skimage.segmentation import relabel_sequential\n","    >>> label_field = np.array([1, 1, 5, 5, 8, 99, 42])\n","    >>> relab, fw, inv = relabel_sequential(label_field)\n","    >>> relab\n","    array([1, 1, 2, 2, 3, 5, 4])\n","    >>> fw\n","    array([0, 1, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5])\n","    >>> inv\n","    array([ 0,  1,  5,  8, 42, 99])\n","    >>> (fw[label_field] == relab).all()\n","    True\n","    >>> (inv[relab] == label_field).all()\n","    True\n","    >>> relab, fw, inv = relabel_sequential(label_field, offset=5)\n","    >>> relab\n","    array([5, 5, 6, 6, 7, 9, 8])\n","    \"\"\"\n","    offset = int(offset)\n","    if offset <= 0:\n","        raise ValueError(\"Offset must be strictly positive.\")\n","    if np.min(label_field) < 0:\n","        raise ValueError(\"Cannot relabel array that contains negative values.\")\n","    max_label = int(label_field.max()) # Ensure max_label is an integer\n","    if not np.issubdtype(label_field.dtype, np.integer):\n","        new_type = np.min_scalar_type(max_label)\n","        label_field = label_field.astype(new_type)\n","    labels = np.unique(label_field)\n","    labels0 = labels[labels != 0]\n","    new_max_label = offset - 1 + len(labels0)\n","    new_labels0 = np.arange(offset, new_max_label + 1)\n","    output_type = label_field.dtype\n","    required_type = np.min_scalar_type(new_max_label)\n","    if np.dtype(required_type).itemsize > np.dtype(label_field.dtype).itemsize:\n","        output_type = required_type\n","    forward_map = np.zeros(max_label + 1, dtype=output_type)\n","    forward_map[labels0] = new_labels0\n","    inverse_map = np.zeros(new_max_label + 1, dtype=output_type)\n","    inverse_map[offset:] = labels0\n","    relabeled = forward_map[label_field]\n","    return relabeled, forward_map, inverse_map\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Drt2bI08JFwc"},"source":["## **5.1. Inspection of the loss function**\n","---\n","\n","<font size = 4>It is good practice to evaluate the training progress by looking at the training loss over training epochs. For more information on this, see for example [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols *et al.*\n","\n","<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target.\n","\n","<font size = 4>During training values should decrease before reaching a minimal value which does not decrease further even after more training.\n","\n","<font size = 4>Decreasing **Training loss** and **Validation loss** indicates that training is still necessary and increasing the `number_of_epochs` is recommended. Note that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0W3iPXWBuuDS","scrolled":false,"cellView":"form"},"source":["#@markdown ##Plot loss function\n","\n","# Stop the trainer if it exists\n","instance_exist = True\n","try:\n","  trainer = InteractiveTrainer.get_instance()\n","except:\n","  instance_exist = False\n","\n","if Use_the_current_trained_model and instance_exist:\n","\n","  trainer = InteractiveTrainer.get_instance()\n","  reports = trainer.get_reports()\n","  \n","  loss = [report['loss'] for report in reports]\n","\n","  plt.figure(figsize=(15,10))\n","\n","  plt.subplot(2,1,1)\n","  plt.plot(loss, label='Training loss')\n","  plt.title('Training loss vs. epoch number (linear scale)')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch number')\n","\n","\n","  plt.subplot(2,1,2)\n","  plt.semilogy(loss, label='Training loss')\n","  plt.title('Training loss vs. epoch number (log scale)')\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch number')\n","  # plt.savefig(os.path.join(QC_model_path,'Quality Control/lossCurvePlots.png'), bbox_inches='tight', pad_inches=0)\n","  plt.show()\n","\n","else:\n","  print(bcolors.WARNING+\"Loss curves can currently only be obtained from a currently trained model.\"+bcolors.NORMAL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YZMyEUreRsD_"},"source":["## **5.2. Error mapping and quality metrics estimation**\n","---\n","<font size = 4>This section will calculate the Intersection over Union score for all the images provided in the `Source_QC_folder` and `Target_QC_folder` ! The result for one of the image will also be displayed.\n","\n","<font size = 4>The **Intersection over Union** (IoU) metric is a method that can be used to quantify the percent overlap between the target mask and your prediction output. **Therefore, the closer to 1, the better the performance.** This metric can be used to assess the quality of your model to accurately predict nuclei. \n","\n","<font size = 4>Here, the IoU is both calculated over the whole image and on a per-object basis. The value displayed below is the IoU value calculated over the entire image. The IoU value calculated on a per-object basis is used to calculate the other metrics displayed.\n","\n","<font size = 4>“n_true” refers to the number of objects present in the ground truth image. “n_pred” refers to the number of objects present in the predicted image. \n","\n","<font size = 4>When a segmented object has an IoU value above 0.5 (compared to the corresponding ground truth), it is then considered a true positive. The number of “**true positives**” is available in the table below. The number of “false positive” is then defined as  “**false positive**” = “n_pred” - “true positive”. The number of “false negative” is defined as “false negative” = “n_true” - “true positive”.\n","\n","<font size = 4>The mean_matched_score is the mean IoUs of matched true positives. The mean_true_score is the mean IoUs of matched true positives but normalized by the total number of ground truth objects. The panoptic_quality is calculated as described by [Kirillov et al. 2019](https://arxiv.org/abs/1801.00868).\n","\n","<font size = 4>For more information about the other metric displayed, please consult the SI of the paper describing ZeroCostDL4Mic.\n","\n","<font size = 4> The results can be found in the \"*Quality Control*\" folder which is located inside your `model_folder`."]},{"cell_type":"code","metadata":{"id":"b8gyZwoERt58","cellView":"form"},"source":["#@markdown ##Choose the folders that contain your Quality Control dataset\n","\n","Source_QC_folder = \"\" #@param{type:\"string\"}\n","Target_QC_folder = \"\" #@param{type:\"string\"}\n","\n","\n","\n","#@markdown ### Segmentation parameters:\n","Object_diameter =  0#@param {type:\"number\"}\n","\n","Flow_threshold = 0.4 #@param {type:\"slider\", min:0.1, max:1.1, step:0.1}\n","Cell_probability_threshold=0 #@param {type:\"slider\", min:-6, max:6, step:1}\n","\n","if Object_diameter is 0:\n","  Object_diameter = None\n","  print(\"The cell size will be estimated automatically for each image\")\n","\n","\n","# Find the number of channel in the input image\n","\n","random_choice = random.choice(os.listdir(Source_QC_folder))\n","x = io.imread(Source_QC_folder+\"/\"+random_choice)\n","n_channel = 1 if x.ndim == 2 else x.shape[-1]\n","\n","\n","channels=[0,0]\n","QC_model_folder = os.path.join(Model_folder,Model_name)\n","QC_model_path = os.path.join(QC_model_folder, 'final')\n","QC_model_name = Model_name\n","\n","#Create a quality control Folder and check if the folder already exist\n","if os.path.exists(QC_model_folder+\"/Quality Control\") == False:\n","  os.makedirs(QC_model_folder+\"/Quality Control\")\n","\n","if os.path.exists(QC_model_folder+\"/Quality Control/Prediction\"):\n","  rmtree(QC_model_folder+\"/Quality Control/Prediction\")\n","os.makedirs(QC_model_folder+\"/Quality Control/Prediction\")\n","\n","\n","model = models.CellposeModel(gpu=True, pretrained_model=QC_model_path, torch=True, diam_mean=30.0, net_avg=True, device=None, residual_on=True, style_on=True, concatenation=False)\n","\n","\n","# Here we need to make predictions\n","\n","for name in os.listdir(Source_QC_folder):\n","  \n","  print(\"Performing prediction on: \"+name)\n","  image = io.imread(Source_QC_folder+\"/\"+name) \n","\n","  short_name = os.path.splitext(name)\n","  masks, flows, styles = model.eval(image, diameter=Object_diameter, flow_threshold=Flow_threshold,cellprob_threshold=Cell_probability_threshold, channels=channels)\n","            \n","  os.chdir(QC_model_folder+\"/Quality Control/Prediction\")\n","  imsave(str(short_name[0])+\".tif\", masks, compress=ZIP_DEFLATED)  \n","  \n","# Here we start testing the differences between GT and predicted masks\n","\n","with open(QC_model_folder+\"/Quality Control/Quality_Control for \"+QC_model_name+\".csv\", \"w\", newline='') as file:\n","  writer = csv.writer(file, delimiter=\",\")\n","  writer.writerow([\"image\",\"Prediction v. GT Intersection over Union\", \"false positive\", \"true positive\", \"false negative\", \"precision\", \"recall\", \"accuracy\", \"f1 score\", \"n_true\", \"n_pred\", \"mean_true_score\", \"mean_matched_score\", \"panoptic_quality\"])  \n","\n","# define the images\n","\n","  for n in os.listdir(Source_QC_folder):\n","    \n","    if not os.path.isdir(os.path.join(Source_QC_folder,n)):\n","      print('Running QC on: '+n)\n","      test_input = io.imread(os.path.join(Source_QC_folder,n))\n","      test_prediction = io.imread(os.path.join(QC_model_folder+\"/Quality Control/Prediction\",n))\n","      test_ground_truth_image = io.imread(os.path.join(Target_QC_folder, n))\n","\n","      # Calculate the matching (with IoU threshold `thresh`) and all metrics\n","\n","      stats = matching(test_ground_truth_image, test_prediction, thresh=0.5)\n","      \n","\n","      #Convert pixel values to 0 or 255\n","      test_prediction_0_to_255 = test_prediction\n","      test_prediction_0_to_255[test_prediction_0_to_255>0] = 255\n","\n","      #Convert pixel values to 0 or 255\n","      test_ground_truth_0_to_255 = test_ground_truth_image\n","      test_ground_truth_0_to_255[test_ground_truth_0_to_255>0] = 255\n","\n","\n","      # Intersection over Union metric\n","\n","      intersection = np.logical_and(test_ground_truth_0_to_255, test_prediction_0_to_255)\n","      union = np.logical_or(test_ground_truth_0_to_255, test_prediction_0_to_255)\n","      iou_score =  np.sum(intersection) / np.sum(union)\n","      writer.writerow([n, str(iou_score), str(stats.fp), str(stats.tp), str(stats.fn), str(stats.precision), str(stats.recall), str(stats.accuracy), str(stats.f1), str(stats.n_true), str(stats.n_pred), str(stats.mean_true_score), str(stats.mean_matched_score), str(stats.panoptic_quality)])\n","\n","\n","\n","df = pd.read_csv (QC_model_folder+\"/Quality Control/Quality_Control for \"+QC_model_name+\".csv\")\n","print(tabulate(df, headers='keys', tablefmt='psql'))\n","\n","\n","\n","# ------------- For display ------------\n","print('--------------------------------------------------------------')\n","@interact\n","def show_QC_results(file = os.listdir(Source_QC_folder)):\n","  \n","\n","  plt.figure(figsize=(25,5))\n","  if n_channel > 1:\n","    source_image = io.imread(os.path.join(Source_QC_folder, file))\n","  if n_channel == 1:\n","    source_image = io.imread(os.path.join(Source_QC_folder, file), as_gray = True)\n","\n","  target_image = io.imread(os.path.join(Target_QC_folder, file), as_gray = True)\n","  prediction = io.imread(QC_model_folder+\"/Quality Control/Prediction/\"+file, as_gray = True)\n","\n","  stats = matching(prediction, target_image, thresh=0.5)\n","\n","  target_image_mask = np.empty_like(target_image)\n","  target_image_mask[target_image > 0] = 255\n","  target_image_mask[target_image == 0] = 0\n","  \n","  prediction_mask = np.empty_like(prediction)\n","  prediction_mask[prediction > 0] = 255\n","  prediction_mask[prediction == 0] = 0\n","\n","  intersection = np.logical_and(target_image_mask, prediction_mask)\n","  union = np.logical_or(target_image_mask, prediction_mask)\n","  iou_score =  np.sum(intersection) / np.sum(union)\n","\n","  norm = simple_norm(source_image, percent = 99)\n","\n","  #Input\n","  plt.subplot(1,4,1)\n","  plt.axis('off')\n","  if n_channel > 1:\n","    plt.imshow(source_image)\n","  if n_channel == 1:\n","    plt.imshow(source_image, aspect='equal', norm=norm, cmap='magma', interpolation='nearest')\n","  plt.title('Input')\n","\n","  #Ground-truth\n","  plt.subplot(1,4,2)\n","  plt.axis('off')\n","  plt.imshow(target_image_mask, aspect='equal', cmap='Greens')\n","  plt.title('Ground Truth')\n","\n","  #Prediction\n","  plt.subplot(1,4,3)\n","  plt.axis('off')\n","  plt.imshow(prediction_mask, aspect='equal', cmap='Purples')\n","  plt.title('Prediction')\n","\n","  #Overlay\n","  plt.subplot(1,4,4)\n","  plt.axis('off')\n","  plt.imshow(target_image_mask, cmap='Greens')\n","  plt.imshow(prediction_mask, alpha=0.5, cmap='Purples')\n","  plt.title('Ground Truth and Prediction, Intersection over Union:'+str(round(iou_score,3 )));\n","  plt.savefig(QC_model_folder+'/Quality Control/QC_example_data.png',bbox_inches='tight',pad_inches=0)\n","\n","\n","# full_QC_model_path = QC_model_folder+'/'\n","# qc_pdf_export()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JqAGDvAI7ALU"},"source":["# **6. Using the trained model**\n","\n","---\n","\n","<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"Lf9YM22iIlzv"},"source":["## **6.1 Generate prediction(s) from unseen dataset**\n","---\n","\n","<font size = 4>The current trained model (from section 4.) can now be used to process images. If an older model needs to be used, please untick the **Use_the_current_trained_model** box and enter the model's name and path to use. Predicted output images are saved in your **Prediction_folder** folder as restored image stacks (ImageJ-compatible TIFF images).\n","\n","<font size = 4>**`Data_folder_prediction`:** This folder should contain the images that you want to predict using the network that you will train.\n","\n","<font size = 4>**`Result_folder`:** This folder is where the results from the predictions will be saved.\n","\n","<font size = 4>**`Flow_threshold`:** This parameter controls the maximum allowed error of the flows for each mask. Increase this threshold if cellpose is not returning as many masks as you'd expect. Similarly, decrease this threshold if cellpose is returning too many ill-shaped masks. **Default value: 0.4**\n","\n","<font size = 4>**`Cell_probability_threshold`:** The pixels greater than the Cell_probability_threshold are used to run dynamics and determine masks.  Decrease this threshold if cellpose is not returning as many masks as you'd expect. Similarly, increase this threshold if cellpose is returning too many masks, particularly from dim areas. **Default value: 0.0**"]},{"cell_type":"code","metadata":{"id":"Yj8M-RD37RQA","cellView":"form"},"source":["# -------------------------------------------------- \n","#@markdown ###Data parameters\n","Data_folder_prediction = \"\" #@param {type:\"string\"}\n","Result_folder = \"\" #@param {type:\"string\"}\n","\n","#@markdown ### Model parameters\n","#@markdown Do you want to use the model you just trained?\n","Use_the_current_trained_model = False #@param {type:\"boolean\"}\n","#@markdown Otherwise, please provide path to the model folder below\n","prediction_model_path = \"\" #@param {type:\"string\"}\n","\n","#@markdown ### Segmentation parameters:\n","Object_diameter =  0#@param {type:\"number\"}\n","Flow_threshold = 0.4 #@param {type:\"slider\", min:0.1, max:1.1, step:0.1}\n","Cell_probability_threshold=0 #@param {type:\"slider\", min:-6, max:6, step:1}\n","\n","# -------------------------------------------------- \n","# TODO: allow to run on other file formats\n","# prediction_image_list = glob.glob(Data_folder_prediction+\"/*.jpg\")\n","# n_files = len(prediction_image_list)\n","\n","# filenames_list = []\n","# for name in os.listdir(Data_folder_prediction):\n","#   if os.path.isfile(name):\n","#     filenames_list.append(os.path.splitext(name)[0])\n","\n","# filenames_list = []\n","# for name in prediction_image_list:\n","#   filenames_list.append(os.path.splitext(os.path.basename(name))[0])\n","\n","extension_list = ['*.jpg', '*.tif', '*.png']\n","prediction_image_list, filenames_list = get_image_list(Data_folder_prediction, extension_list)\n","\n","n_files = len(prediction_image_list)\n","print(\"Total number of files: \"+str(n_files))\n","\n","if Use_the_current_trained_model:\n","  prediction_model_path = os.path.join(Model_folder, Model_name)\n","\n","\n","model_path = os.path.join(prediction_model_path, \"final\")\n","# TODO: Check the line below for file compatibility\n","channels=[0,0] \n","model = models.CellposeModel(gpu=True, pretrained_model=model_path, torch=False, diam_mean=30.0, net_avg=True, device=None, residual_on=True, style_on=True, concatenation=False)\n","\n","\n","if (Object_diameter == 0):\n","  Object_diameter = None\n","\n","masks_list = []\n","for i in tqdm(range(n_files)):\n","  img = io.imread(prediction_image_list[i])\n","  masks, flows, styles = model.eval(img, diameter=Object_diameter, flow_threshold=Flow_threshold,cellprob_threshold=Cell_probability_threshold, channels=channels)\n","  imsave(os.path.join(Result_folder, filenames_list[i]+'.tif'), masks)\n","\n","\n","\n","\n","# ------------- For display ------------\n","print('--------------------------------------------------------------')\n","@interact\n","def show_labels(filename = filenames_list):\n","  plt.figure(figsize=(13,10))\n","\n","  img = io.imread(os.path.join(Data_folder_prediction, filename))\n","  mask = io.imread(os.path.join(Result_folder, filename+'.tif'))\n","\n","  plt.subplot(121)\n","  plt.imshow(img, cmap='gray', interpolation='nearest')\n","  plt.title('Source image')\n","  plt.axis('off')\n","  plt.subplot(122)\n","  plt.imshow(mask, cmap='nipy_spectral', interpolation='nearest')\n","  plt.title('Label')\n","  plt.axis('off');\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D0I9oX82QDk6"},"source":["## **6.2. Download your predictions**\n","---\n","\n","<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."]},{"cell_type":"markdown","metadata":{"id":"w4sH7tKyKQ2z"},"source":["# **7. Version log**\n","\n","---\n","<font size = 4>**v1.13.1**: \n","\n","* The notebook now can take a folder containing masks for pre-annotated data. It creates the corresponding json file to be imported into the trainer.\n","\n","---\n","<font size = 4>**v1.13**: \n","\n","* The section 1 and 2 are now swapped for better export of *requirements.txt*. \n","* This version also now includes built-in version check and the version log that you're reading now. \n","* This version also specifically pulls StarDist packages version 0.6.2, due to current incompatibilities with the newest versions.\n","* Better data input compatibilities.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"2nfGReX7QJnE"},"source":["---\n","#**Thank you for using Interactive segmentation - Cellpose 2D!**"]}]}